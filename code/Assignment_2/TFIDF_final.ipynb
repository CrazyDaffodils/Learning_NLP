{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12bd1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b823522",
   "metadata": {},
   "source": [
    "### Data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78912414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 83032: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 154657: expected 6 fields, saw 7\\n'\n",
      "b'Skipping line 323916: expected 6 fields, saw 7\\n'\n",
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/data.tsv', sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7156e816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133273</td>\n",
       "      <td>213221</td>\n",
       "      <td>213222.0</td>\n",
       "      <td>How is the life of a math student? Could you d...</td>\n",
       "      <td>Which level of prepration is enough for the ex...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>402555</td>\n",
       "      <td>536040</td>\n",
       "      <td>536041.0</td>\n",
       "      <td>How do I control my horny emotions?</td>\n",
       "      <td>How do you control your horniness?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>360472</td>\n",
       "      <td>364011</td>\n",
       "      <td>490273.0</td>\n",
       "      <td>What causes stool color to change to yellow?</td>\n",
       "      <td>What can cause stool to come out as little balls?</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150662</td>\n",
       "      <td>155721</td>\n",
       "      <td>7256.0</td>\n",
       "      <td>What can one do after MBBS?</td>\n",
       "      <td>What do i do after my MBBS ?</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>183004</td>\n",
       "      <td>279958</td>\n",
       "      <td>279959.0</td>\n",
       "      <td>Where can I find a power outlet for my laptop ...</td>\n",
       "      <td>Would a second airport in Sydney, Australia be...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    qid1      qid2  \\\n",
       "0  133273  213221  213222.0   \n",
       "1  402555  536040  536041.0   \n",
       "2  360472  364011  490273.0   \n",
       "3  150662  155721    7256.0   \n",
       "4  183004  279958  279959.0   \n",
       "\n",
       "                                           question1  \\\n",
       "0  How is the life of a math student? Could you d...   \n",
       "1                How do I control my horny emotions?   \n",
       "2       What causes stool color to change to yellow?   \n",
       "3                        What can one do after MBBS?   \n",
       "4  Where can I find a power outlet for my laptop ...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  Which level of prepration is enough for the ex...           0.0  \n",
       "1                 How do you control your horniness?           1.0  \n",
       "2  What can cause stool to come out as little balls?           0.0  \n",
       "3                       What do i do after my MBBS ?           1.0  \n",
       "4  Would a second airport in Sydney, Australia be...           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b11bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363192 entries, 0 to 363191\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            363192 non-null  object \n",
      " 1   qid1          363192 non-null  object \n",
      " 2   qid2          363185 non-null  float64\n",
      " 3   question1     363181 non-null  object \n",
      " 4   question2     363180 non-null  object \n",
      " 5   is_duplicate  363180 non-null  float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 16.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bacfd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with missing entries in queries, responses or labels\n",
    "df = df.dropna(axis=0, subset=('question1','question2','is_duplicate' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ef301a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 363177 entries, 0 to 363191\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   id            363177 non-null  object \n",
      " 1   qid1          363177 non-null  object \n",
      " 2   qid2          363177 non-null  float64\n",
      " 3   question1     363177 non-null  object \n",
      " 4   question2     363177 non-null  object \n",
      " 5   is_duplicate  363177 non-null  float64\n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 19.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660db93",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d36615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#lower casing\n",
    "def lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "#remove numbers\n",
    "def remove_numbers(text):\n",
    "    output = re.sub(r'\\d+', '', text)\n",
    "    return output\n",
    "\n",
    "# remove punctuation\n",
    "import string\n",
    "def remove_punctuation(text):\n",
    "    text_p = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_p\n",
    "\n",
    "#tokenize text\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in text if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "#remove single character tokens\n",
    "\n",
    "def remove_single_characters(text):\n",
    "    filtered_words = [word for word in text if len(word) > 1]\n",
    "    return filtered_words\n",
    "\n",
    "#stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmed = [stemmer.stem(word) for word in text]\n",
    "    return stemmed\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd7bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self,df_column, steps):\n",
    "        self.df_column = df_column\n",
    "        self.steps = steps\n",
    "    \n",
    "        if 'lower_case' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: lower_case(x))\n",
    "        \n",
    "        if 'remove_numbers' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: remove_numbers(x))\n",
    "        \n",
    "        if 'remove_punctuation' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: remove_punctuation(x))           \n",
    "        \n",
    "        if 'tokenize' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: tokenize(x))\n",
    "        \n",
    "        if 'stopwords' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: remove_stopwords(x))\n",
    "            \n",
    "        if 'single_characters' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: remove_single_characters(x))\n",
    "        \n",
    "        if 'stemming' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: stem_words(x))\n",
    "            \n",
    "        if 'lemmatize' in self.steps:\n",
    "            self.df_column = self.df_column.apply(lambda x: lemmatize(x))    \n",
    "            \n",
    "        return self.df_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8ba2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b522311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess columns 'question1' and 'question2'\n",
    "steps = ['lower_case','remove_numbers','remove_punctuation',\n",
    "        'tokenize','stopwords','single_characters','stemming']\n",
    "processor = Preprocessor()\n",
    "df['question1'] = processor.preprocess(df['question1'] ,steps)\n",
    "df['question2'] = processor.preprocess(df['question2'] ,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb147fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 363177 entries, 0 to 363176\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   index         363177 non-null  int64  \n",
      " 1   id            363177 non-null  object \n",
      " 2   qid1          363177 non-null  object \n",
      " 3   qid2          363177 non-null  float64\n",
      " 4   question1     363177 non-null  object \n",
      " 5   question2     363177 non-null  object \n",
      " 6   is_duplicate  363177 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(4)\n",
      "memory usage: 19.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1e2d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>question1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536040</td>\n",
       "      <td>[control, horni, emot]</td>\n",
       "      <td>536041.0</td>\n",
       "      <td>[control, horni]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>155721</td>\n",
       "      <td>[one, mbb]</td>\n",
       "      <td>7256.0</td>\n",
       "      <td>[mbb]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>147570</td>\n",
       "      <td>[best, self, help, book, read, chang, life]</td>\n",
       "      <td>787.0</td>\n",
       "      <td>[top, self, help, book, read]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71243</td>\n",
       "      <td>[hillari, clinton, polici, toward, india, beco...</td>\n",
       "      <td>177376.0</td>\n",
       "      <td>[hilari, clinton, polici, toward, india, becom...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22332</td>\n",
       "      <td>[best, book, studi, tensor, gener, rel, basic]</td>\n",
       "      <td>22333.0</td>\n",
       "      <td>[best, book, tensor, calculu]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid1                                          question1      qid2  \\\n",
       "0  536040                             [control, horni, emot]  536041.0   \n",
       "1  155721                                         [one, mbb]    7256.0   \n",
       "2  147570        [best, self, help, book, read, chang, life]     787.0   \n",
       "3   71243  [hillari, clinton, polici, toward, india, beco...  177376.0   \n",
       "4   22332     [best, book, studi, tensor, gener, rel, basic]   22333.0   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0                                   [control, horni]           1.0  \n",
       "1                                              [mbb]           1.0  \n",
       "2                      [top, self, help, book, read]           1.0  \n",
       "3  [hilari, clinton, polici, toward, india, becom...           1.0  \n",
       "4                      [best, book, tensor, calculu]           1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe of top 100 queries that have label = 1\n",
    "queries = df[df['is_duplicate'] == 1][:100]\n",
    "queries = queries[['qid1','question1','qid2','question2', 'is_duplicate']]\n",
    "queries = queries.reset_index(drop=True)\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8d84241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility class for printing\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66827ac4",
   "metadata": {},
   "source": [
    "### Sentence matching with TFIDF -  sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "605c1d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-bfd566458b64>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus.drop_duplicates(subset='qid2', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSklearn implementation\u001b[0m\n",
      "\n",
      "Accuracy of sentence matching with TFIDF \"l1\" normalization method...\n",
      "\n",
      "Retrieving correct response in top 2 matched responses: 16.0 %\n",
      "Retrieving correct response in top 5 matched responses: 32.0 %\n",
      "\n",
      "\n",
      "Accuracy of sentence matching with TFIDF \"l2\" normalization method...\n",
      "\n",
      "Retrieving correct response in top 2 matched responses: 45.0 %\n",
      "Retrieving correct response in top 5 matched responses: 61.0 %\n"
     ]
    }
   ],
   "source": [
    "corpus = df[['qid2','question2']]\n",
    "\n",
    "corpus.drop_duplicates(subset='qid2', inplace=True)\n",
    "\n",
    "#sklearn tfidf - l1 normalization method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "train_questions = list(corpus['question2'])\n",
    "query_questions = list(queries['question1'])\n",
    "\n",
    "\n",
    "tf1 = TfidfVectorizer(tokenizer=None,preprocessor=None, analyzer = (lambda x:x),lowercase=False, norm='l1')\n",
    "X_train = tf1.fit_transform(train_questions)\n",
    "X_test = tf1.transform(query_questions)\n",
    "\n",
    "matched = np.array((X_test@X_train.T).todense())\n",
    "matched_sorted = np.zeros(matched.shape, dtype='int')\n",
    "\n",
    "sktop2_l1 = []\n",
    "sktop5_l1 = []\n",
    "\n",
    "for i in range(matched.shape[0]):\n",
    "    matched_sorted[i,:] = matched[i,:].argsort()[::-1]\n",
    "    top2ind = matched_sorted[i,:2]\n",
    "    top5ind = matched_sorted[i,:5]\n",
    "    top2docs = [corpus.iloc[c,0] for c in top2ind]\n",
    "    top5docs = [corpus.iloc[c,0] for c in top5ind]\n",
    "    if queries.iloc[i,2] in top2docs:\n",
    "        sktop2_l1.append(1)\n",
    "    else:\n",
    "        sktop2_l1.append(0)\n",
    "    if queries.iloc[i,2] in top5docs:\n",
    "        sktop5_l1.append(1)\n",
    "    else:\n",
    "        sktop5_l1.append(0)\n",
    "\n",
    "tf2 = TfidfVectorizer(tokenizer=None,preprocessor=None, analyzer = (lambda x:x),lowercase=False, norm='l2')\n",
    "X_train2 = tf2.fit_transform(train_questions)\n",
    "X_test2 = tf2.transform(query_questions)\n",
    "matched2 = np.array((X_test2@X_train2.T).todense())\n",
    "matched_sorted2 = np.zeros(matched2.shape, dtype='int')\n",
    "\n",
    "sktop2_l2 = []\n",
    "sktop5_l2 = []\n",
    "\n",
    "for i in range(matched2.shape[0]):\n",
    "    matched_sorted2[i,:] = matched2[i,:].argsort()[::-1]\n",
    "    top2ind = matched_sorted2[i,:2]\n",
    "    top5ind = matched_sorted2[i,:5]\n",
    "    top2docs = [corpus.iloc[c,0] for c in top2ind]\n",
    "    top5docs = [corpus.iloc[c,0] for c in top5ind]\n",
    "    if queries.iloc[i,2] in top2docs:\n",
    "        sktop2_l2.append(1)\n",
    "    else:\n",
    "        sktop2_l2.append(0)\n",
    "    if queries.iloc[i,2] in top5docs:\n",
    "        sktop5_l2.append(1)\n",
    "    else:\n",
    "        sktop5_l2.append(0)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "sktop2_l1_acc = accuracy_score(queries['is_duplicate'], sktop2_l1)\n",
    "sktop5_l1_acc = accuracy_score(queries['is_duplicate'], sktop5_l1)\n",
    "sktop2_l2_acc = accuracy_score(queries['is_duplicate'], sktop2_l2)\n",
    "sktop5_l2_acc = accuracy_score(queries['is_duplicate'], sktop5_l2)\n",
    "print(color.BOLD+'Sklearn implementation'+color.END)\n",
    "print()\n",
    "print('Accuracy of sentence matching with TFIDF \"l1\" normalization method...')\n",
    "print()\n",
    "print('Retrieving correct response in top 2 matched responses: {:.1f} %'.format(sktop2_l1_acc*100))\n",
    "print('Retrieving correct response in top 5 matched responses: {:.1f} %'.format(sktop5_l1_acc*100))\n",
    "print()\n",
    "print()\n",
    "print('Accuracy of sentence matching with TFIDF \"l2\" normalization method...')\n",
    "print()\n",
    "print('Retrieving correct response in top 2 matched responses: {:.1f} %'.format(sktop2_l2_acc*100))\n",
    "print('Retrieving correct response in top 5 matched responses: {:.1f} %'.format(sktop5_l2_acc*100))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f91c78",
   "metadata": {},
   "source": [
    "### Sentence Matching using TF-IDF method - implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8caf547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50010"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a vocabulary list from tokens in responses\n",
    "q2vocab = df['question2'].tolist()\n",
    "q2vocab = [item for sublist in q2vocab for item in sublist]\n",
    "q2vocab = list(set(q2vocab))\n",
    "len(q2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33e58f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nemo',\n",
       "  [303233.0,\n",
       "   513861.0,\n",
       "   496678.0,\n",
       "   413610.0,\n",
       "   172205.0,\n",
       "   246991.0,\n",
       "   413781.0,\n",
       "   456023.0]),\n",
       " ('bitdefend', [114823.0, 71175.0, 11719.0, 26538.0, 26539.0])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary - word in vocabulary: list of response ids (q2id) it is found in\n",
    "Word_Doc_ids = {}\n",
    "for word in q2vocab:\n",
    "    Word_Doc_ids[word] = []\n",
    "for i in range(len(df)):\n",
    "    for word in df.loc[i,'question2']:\n",
    "        Word_Doc_ids[word].append(df.loc[i,'qid2'])\n",
    "\n",
    "#create set to drop duplicate values of doc_ids\n",
    "for word in q2vocab:\n",
    "    Word_Doc_ids[word] = list(set(Word_Doc_ids[word]))\n",
    "    \n",
    "    \n",
    "list(Word_Doc_ids.items())[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03d6a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique responses: 273121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ski', 9.116692476478114),\n",
       " ('fprove', 11.824537664272047),\n",
       " ('chinapakisthan', 11.824537664272047),\n",
       " ('nemo', 10.320511524763331),\n",
       " ('bitdefend', 10.725954665792823)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create Inverse Document_frequency (IDF) dictionary - word in vocabulary : IDF\n",
    "#use idf formula as per sklearn implementation - for smoothing add 1 to numerator and denominator\n",
    "\n",
    "N = df['qid2'].nunique()\n",
    "print('Number of unique responses: {}'.format(N))\n",
    "\n",
    "IDF = {}\n",
    "for word in Word_Doc_ids.keys():\n",
    "    IDF[word] = np.log((1+N/(len(Word_Doc_ids[word]) +1))+1)\n",
    "list(IDF.items())[:5]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a236517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(213222.0, ['level', 'preprat', 'enough', 'exam', 'jlpt']),\n",
       " (536041.0, ['control', 'horni'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dictionary of list of words in each response - q2id : words\n",
    "doc_words = dict(zip(df['qid2'],df['question2']))\n",
    "\n",
    "list(doc_words.items())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ad6f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to calculate  norm - to apply 'l1' or 'l2' normalization for calculation of term frequency\n",
    "\n",
    "def norm(list_of_words, normalization_method = 'l1'):\n",
    "    word_vec = np.array([list_of_words.count(word) for word in set(list_of_words)])\n",
    "    if normalization_method == 'l1':\n",
    "        norm = np.linalg.norm(word_vec,ord=1)\n",
    "    else:\n",
    "        norm = np.linalg.norm(word_vec, ord=2)\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9531dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where keys are words in the vocabulary, \n",
    "#values are dictionary of key:values as q2id : term frequency for corresponding word\n",
    "#nested dictionary - word: {{q2id1: tf}, {q2id2: tf}, ....}\n",
    "#use 'l1' norm to normalize term frequency\n",
    "\n",
    "inverted_tf_l1 = {}\n",
    "for word in q2vocab:\n",
    "    word_tf = {}\n",
    "    for doc in Word_Doc_ids[word]:\n",
    "        tf = doc_words[doc].count(word)/norm(doc_words[doc],'l1')\n",
    "        word_tf[doc] = tf\n",
    "    inverted_tf_l1[word] = word_tf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a390f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use 'l2' norm to normalize term frequency\n",
    "\n",
    "inverted_tf_l2 = {}\n",
    "for word in q2vocab:\n",
    "    word_tf = {}\n",
    "    for doc in Word_Doc_ids[word]:\n",
    "        tf = doc_words[doc].count(word)/norm(doc_words[doc],'l2')\n",
    "        word_tf[doc] = tf\n",
    "    inverted_tf_l2[word] = word_tf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f6e05",
   "metadata": {},
   "source": [
    "#### Creating inverted file for retreiving tfidf values - term frequencies normalized by either 'l1' or 'l2' norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be2b845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary where keys are words in the vocab, \n",
    "#values are dictinary of key:values as doc_id:tfidf for corresponding word\n",
    "#nested dictionary - word: {{doc_id1: tf}, {doc_id2: tf}, ....}\n",
    "\n",
    "inverted_tfidf_l1 = {}\n",
    "for word in q2vocab:\n",
    "    doc_tfidfs = []\n",
    "    for tup in list(inverted_tf_l1[word].items()):\n",
    "        tup = list(tup)\n",
    "        tfidf = tup[1]*IDF[word]\n",
    "        tup[1] = tfidf\n",
    "        doc_tfidfs.append(tup)\n",
    "    inverted_tfidf_l1[word] = doc_tfidfs  \n",
    "    \n",
    "    \n",
    "inverted_tfidf_l2 = {}\n",
    "for word in q2vocab:\n",
    "    doc_tfidfs = []\n",
    "    for tup in list(inverted_tf_l2[word].items()):\n",
    "        tup = list(tup)\n",
    "        tfidf = tup[1]*IDF[word]\n",
    "        tup[1] = tfidf\n",
    "        doc_tfidfs.append(tup)\n",
    "    inverted_tfidf_l2[word] = doc_tfidfs     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc636be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(inverted_tfidf_l1)):\n",
    "    inverted_tfidf_l1[q2vocab[i]] = {x[0]:x[1] for x in inverted_tfidf_l1[q2vocab[i]]}\n",
    "    \n",
    "for i in range(len(inverted_tfidf_l2)):\n",
    "    inverted_tfidf_l2[q2vocab[i]] = {x[0]:x[1] for x in inverted_tfidf_l2[q2vocab[i]]}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4827c290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nemo',\n",
       "  {303233.0: 1.2900639405954164,\n",
       "   513861.0: 1.7200852541272218,\n",
       "   496678.0: 2.580127881190833,\n",
       "   413610.0: 2.580127881190833,\n",
       "   172205.0: 2.0641023049526663,\n",
       "   246991.0: 1.1467235027514813,\n",
       "   413781.0: 1.4743587892519043,\n",
       "   456023.0: 3.4401705082544436}),\n",
       " ('bitdefend',\n",
       "  {114823.0: 2.1451909331585646,\n",
       "   71175.0: 2.681488666448206,\n",
       "   11719.0: 2.681488666448206,\n",
       "   26538.0: 3.0645584759408067,\n",
       "   26539.0: 1.7876591109654705})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inverted_tfidf_l1.items())[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aea9278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to access unprocessed question from original dataframe\n",
    "def retreive_unprocessed_question(question_qid, question_type):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        question_qid : 'qid1' or 'qid2' \n",
    "        \n",
    "        question_type : 'question1' or 'question2'\n",
    "        \n",
    "   Returns: \n",
    "       unprocessed question : question1 or question2 from original unprocessed dataframe\n",
    "    \"\"\"\n",
    "    if question_type == 'query':\n",
    "        unprocessed_question = unprocessed_df.loc[unprocessed_df['qid1']==question_qid, 'question1'].iloc[0]\n",
    "    else:\n",
    "        unprocessed_question = unprocessed_df.loc[unprocessed_df['qid2']==question_qid, 'question2'].iloc[0]\n",
    "    \n",
    "    return unprocessed_question\n",
    "\n",
    "\n",
    "#define function to retrieve matching responses against one query\n",
    "from collections import Counter\n",
    "def get_tfidfRanked_responses(normalization_method,query_index,number_of_responses, print_results = False ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        normalization_method : 'l1' or 'l2'\n",
    "    \n",
    "        query_index : 0 to 99 - first 100 queries that have label =  1\n",
    "\n",
    "        number_of_responses : top2 or top5 numbers of matching responses against a query\n",
    "        \n",
    "        print_results : If true, prints the unprocessed query, ground truth response and top2 or top5 matching responses obtained by sentence embedding method\n",
    "    \n",
    "    Returns:\n",
    "        matched_responses : list of 'qid2' for ranked number of matching responses\n",
    "    \"\"\"\n",
    "    \n",
    "    query = queries.loc[query_index,'question1']\n",
    "    query_qid = queries.loc[query_index,'qid1']\n",
    "    actual_response_id = queries.loc[query_index,'qid2']\n",
    "    \n",
    "    if normalization_method == 'l1':\n",
    "        inverted_file = inverted_tfidf_l1\n",
    "    else:\n",
    "        inverted_file = inverted_tfidf_l2\n",
    "    \n",
    "    counter_objects = []\n",
    "    for word in query:\n",
    "        if word in inverted_file.keys():\n",
    "            counter_objects.append(Counter(inverted_file[word]))\n",
    "            combined = sum(counter_objects,Counter())\n",
    "            matched_responses = sorted(combined, key = combined.get, reverse=True)[:number_of_responses]\n",
    "    \n",
    "    if print_results:\n",
    "        print(color.BOLD+'Sentence matching with TF-IDF method - implementation from scratch...'+color.END) \n",
    "        print()\n",
    "    \n",
    "        print(color.BOLD+'Query'+color.END)\n",
    "        print(color.RED+retreive_unprocessed_question(query_qid,'query')+color.END)\n",
    "        print()\n",
    "        print(color.BOLD+'Ground truth (actual) matching response'+color.END)\n",
    "        print(color.CYAN +retreive_unprocessed_question(actual_response_id,'response')+color.END)\n",
    "        print()\n",
    "        print(color.BOLD+'Top {} matched responses by TFIDF method, {} normalization'.format(number_of_responses,normalization_method)+color.END)\n",
    "        for response_id in matched_responses:\n",
    "            if response_id == actual_response_id:\n",
    "                print(color.CYAN+retreive_unprocessed_question(response_id,'response')+color.END)\n",
    "            else:\n",
    "                print(retreive_unprocessed_question(response_id,'response'))\n",
    "    else:        \n",
    "        \n",
    "        return matched_responses \n",
    "    \n",
    "    \n",
    "# define function to get matching responses against all queries\n",
    "def all_queries_responses(normalization_method,number_of_responses):\n",
    "    response_labels = []\n",
    "    for i in range(len(queries)):\n",
    "        matched_responses = get_tfidfRanked_responses(normalization_method,i,number_of_responses, print_results = False )\n",
    "        if queries.loc[i,'qid2'] in matched_responses:\n",
    "            response_labels.append(1.0)\n",
    "        else:\n",
    "            response_labels.append(0.0)\n",
    "            \n",
    "    return response_labels\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "143d43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get prediction labels for matched responses using 'l1' or 'l2' normalization methods\n",
    "top2_pred_l1 = all_queries_responses('l1',2)\n",
    "top5_pred_l1 = all_queries_responses('l1',5)\n",
    "top2_pred_l2 = all_queries_responses('l2',2)\n",
    "top5_pred_l2 = all_queries_responses('l2',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5db12b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mImplementation from scratch\u001b[0m\n",
      "\n",
      "Accuracy of sentence matching with TFIDF \"l1\" normalization method...\n",
      "\n",
      "Retrieving correct response in top 2 matched responses: 16.0 %\n",
      "Retrieving correct response in top 5 matched responses: 30.0 %\n",
      "\n",
      "\n",
      "Accuracy of sentence matching with TFIDF \"l2\" normalization method...\n",
      "\n",
      "Retrieving correct response in top 2 matched responses: 47.0 %\n",
      "Retrieving correct response in top 5 matched responses: 58.0 %\n"
     ]
    }
   ],
   "source": [
    "top2_l1 = accuracy_score(queries['is_duplicate'],top2_pred_l1)\n",
    "top5_l1 = accuracy_score(queries['is_duplicate'],top5_pred_l1)\n",
    "top2_l2 = accuracy_score(queries['is_duplicate'],top2_pred_l2)\n",
    "top5_l2 = accuracy_score(queries['is_duplicate'],top5_pred_l2)\n",
    "\n",
    "print(color.BOLD+\"Implementation from scratch\"+color.END)\n",
    "print()\n",
    "print('Accuracy of sentence matching with TFIDF \"l1\" normalization method...')\n",
    "print()\n",
    "print('Retrieving correct response in top 2 matched responses: {:.1f} %'.format(top2_l1*100))\n",
    "print('Retrieving correct response in top 5 matched responses: {:.1f} %'.format(top5_l1*100))\n",
    "print()\n",
    "print()\n",
    "print('Accuracy of sentence matching with TFIDF \"l2\" normalization method...')\n",
    "print()\n",
    "print('Retrieving correct response in top 2 matched responses: {:.1f} %'.format(top2_l2*100))\n",
    "print('Retrieving correct response in top 5 matched responses: {:.1f} %'.format(top5_l2*100))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ab1bc",
   "metadata": {},
   "source": [
    "### View matching responses for selected queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7744aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence matching with TF-IDF method - implementation from scratch...\u001b[0m\n",
      "\n",
      "\u001b[1mQuery\u001b[0m\n",
      "\u001b[91mWhat can one do after MBBS?\u001b[0m\n",
      "\n",
      "\u001b[1mGround truth (actual) matching response\u001b[0m\n",
      "\u001b[96mWhat do i do after my MBBS ?\u001b[0m\n",
      "\n",
      "\u001b[1mTop 5 matched responses by TFIDF method, l1 normalization\u001b[0m\n",
      "What have to Do after MBBS?\n",
      "What can I do after my MBBs?\n",
      "\u001b[96mWhat do i do after my MBBS ?\u001b[0m\n",
      "What should one do and should not do in 40s?\n",
      "How can I do mbbs after doing BDS?\n"
     ]
    }
   ],
   "source": [
    "get_tfidfRanked_responses(normalization_method='l1', query_index=1, number_of_responses=5, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9663365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence matching with TF-IDF method - implementation from scratch...\u001b[0m\n",
      "\n",
      "\u001b[1mQuery\u001b[0m\n",
      "\u001b[91mWhat can one do after MBBS?\u001b[0m\n",
      "\n",
      "\u001b[1mGround truth (actual) matching response\u001b[0m\n",
      "\u001b[96mWhat do i do after my MBBS ?\u001b[0m\n",
      "\n",
      "\u001b[1mTop 5 matched responses by TFIDF method, l2 normalization\u001b[0m\n",
      "What have to Do after MBBS?\n",
      "What can I do after my MBBs?\n",
      "\u001b[96mWhat do i do after my MBBS ?\u001b[0m\n",
      "How can I do mbbs after doing BDS?\n",
      "How should I study in MBBS?\n"
     ]
    }
   ],
   "source": [
    "get_tfidfRanked_responses(normalization_method='l2', query_index=1, number_of_responses=5, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d65240f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence matching with TF-IDF method - implementation from scratch...\u001b[0m\n",
      "\n",
      "\u001b[1mQuery\u001b[0m\n",
      "\u001b[91mWhat are some of the most beautiful houses in the world?\u001b[0m\n",
      "\n",
      "\u001b[1mGround truth (actual) matching response\u001b[0m\n",
      "\u001b[96mWhich are some of the most beautiful houses around the world?\u001b[0m\n",
      "\n",
      "\u001b[1mTop 2 matched responses by TFIDF method, l1 normalization\u001b[0m\n",
      "What is the beauty?\n",
      "Why am I beautiful?\n"
     ]
    }
   ],
   "source": [
    "get_tfidfRanked_responses(normalization_method='l1', query_index=15, number_of_responses=2, print_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c25fbb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence matching with TF-IDF method - implementation from scratch...\u001b[0m\n",
      "\n",
      "\u001b[1mQuery\u001b[0m\n",
      "\u001b[91mWhat are some of the most beautiful houses in the world?\u001b[0m\n",
      "\n",
      "\u001b[1mGround truth (actual) matching response\u001b[0m\n",
      "\u001b[96mWhich are some of the most beautiful houses around the world?\u001b[0m\n",
      "\n",
      "\u001b[1mTop 2 matched responses by TFIDF method, l2 normalization\u001b[0m\n",
      "What are some of the most beautiful houses in the world?\n",
      "\u001b[96mWhich are some of the most beautiful houses around the world?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "get_tfidfRanked_responses(normalization_method='l2', query_index=15, number_of_responses=2, print_results = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
